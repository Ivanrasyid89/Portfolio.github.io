# -*- coding: utf-8 -*-
"""Mobile Price Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DHcnU9xTE9KlNdadPMJM79CjmNuYwp7u

# **IMPORT LIBRARY**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# Split dataset
from sklearn.model_selection import train_test_split
# Standardscaller
from sklearn.preprocessing import StandardScaler, LabelBinarizer
# Cross validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
# Evaluation metric
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, auc
# Tuning parameter
from sklearn.model_selection import GridSearchCV
# XGBOOST
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
# TensorFlow
import tensorflow as tf
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

"""# **IMPORT DATA**

### **Read Data**
"""

data_train = pd.read_csv("train.csv")
data_test = pd.read_csv("test.csv")

data_train.head()

data_test.head()

"""### **Data Information**"""

data_train.info()

data_test.info()

"""# **EXPLORATORY DATA ANALYSIS**

In data exploration, there are 3 important things that need to be done to find out the initial overview of the data, i.e.:
- Checking for missing values
- Data distribution check
- Correlation

## **Missing Value**
"""

data_train.isnull().sum()

"""## **Data Distribution**

### **Box-plot**
"""

# List numerical column
col_num = ['battery_power', 'clock_speed', 'fc', 'int_memory', 'm_dep', 'mobile_wt',
           'n_cores', 'pc', 'px_height', 'px_width', 'ram', 'sc_h', 'talk_time']

plt.figure(figsize=(15,10))
for i, column in enumerate(col_num):
  plt.subplot(4,4,i+1)
  sns.boxplot(data = data_train, y=column, color='skyblue')
  plt.title(column, fontsize = 12)
  plt.xlabel('')
  plt.ylabel('')
  plt.tight_layout();

"""### **Bar-plot**"""

# List categorical column
col_cat = ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi', 'price_range']

colors = ['skyblue', 'salmon', 'lightgreen', 'orange']

for i, column in enumerate(col_cat):
    plt.subplot(2, 4, i + 1)
    data_train[column].value_counts().plot(kind='bar', color=colors)
    plt.title(column, fontsize=12)
    plt.ylabel('')
    plt.tight_layout()

"""## **Correlation**"""

# List numerical column
col_num = ['battery_power', 'clock_speed', 'fc', 'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc',
           'px_height', 'px_width', 'ram', 'sc_h', 'talk_time']
# List categorical column
col_cat = ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi']
# Merge column
variables = col_num + col_cat

# Correlation
cor = data_train[variables + ['price_range']].corr()

# Heatmap
plt.figure(figsize=(20, 15))
sns.heatmap(cor.loc[col_num + ['price_range'], variables], annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('')
plt.show()

"""# **DATA PROCESSING**

Then conduct data processing. In this stage, it is categorized into 3 sections, namely:
- Data standardization
- Feature engineering
- Split data

## **Standardization**
"""

# Function to scaling data
def scale_data(data_train):
    # Make copy data
    scaled_data_train = data_train.copy()
    num_fit = ['battery_power', 'clock_speed', 'fc', 'int_memory', 'm_dep', 'mobile_wt',
               'n_cores', 'pc', 'px_height', 'px_width', 'ram', 'sc_h', 'talk_time']
    # Standardscaler (Z score)
    scaler = StandardScaler()
    scaled_data_train[num_fit] = scaler.fit_transform(scaled_data_train[num_fit])
    return scaled_data_train

scaled_data_train = scale_data(data_train)
print(scaled_data_train)

"""## **Feature Engineering**"""

# Make feature column to numerical and categorical column
feature_columns = []

# Numerical column
num_fit = ['battery_power', 'clock_speed', 'fc', 'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc',
           'px_height', 'px_width', 'ram', 'sc_h', 'talk_time']

for col in num_fit:
  feature_num = tf.feature_column.numeric_column(col)
  feature_columns.append(feature_num)

# Categorical column
cat_fit = ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi']

for col in cat_fit:
    feature_cat = tf.feature_column.categorical_column_with_vocabulary_list(
        key=col,
        vocabulary_list=scaled_data_train[col].unique().tolist()
    )
    feature_cat_onehot = tf.feature_column.indicator_column(feature_cat)
    feature_columns.append(feature_cat_onehot)

"""### **Feature Layer**"""

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

"""### **Change to TensorFlow**"""

# Function to convert dataframe to Tensorflow dataset
def data_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()
  labels = dataframe.pop('price_range')
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe),labels))

  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))

  ds = ds.batch(batch_size=batch_size)

  return ds

"""## **Split Data**

Partitioned data into training data and testing data randomly. The training data is 80% of the total data, while the testing data is 20% of the overall data.

### **Neural Network**
"""

# Partition data for Neural Network #
train, test = train_test_split(scaled_data_train, test_size=0.2, random_state=42)

# Convert dataframe to tfds dataset
train_ds = data_to_dataset(train, shuffle = True, batch_size=32)
test_ds = data_to_dataset(test, shuffle=False, batch_size=32)

X = scale_data(data_train.drop('price_range', axis=1))
y = scaled_data_train['price_range']

X

y

"""### **XGBoost**"""

# Partition data for XGBoost #
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)
print(f"X_train : {X_train.shape}")
print(f"X_test : {X_test.shape}")
print(f"y_train : {y_train.shape}")
print(f"y_test : {y_test.shape}")

X_train

y_train

X_test

y_test

"""# **MODELLING**

The machine learning models used are Neural Network and XGBoost.

### **Neural Network**

This code snippet builds, compiles, and trains a neural network model using TensorFlow's Keras API for a multi-class classification problem with four classes. Key steps include:

- **Building the Model**: Adding layers sequentially including preprocessing, dense, dropout, and regularization layers.
- **Compiling the Model**: Defining the optimizer, loss function, and evaluation metric.
- **Training the Model**: Fitting the model on the training dataset and validating it using the validation dataset over a specified number of epochs.
"""

## Build NN model ##
model_nn = tf.keras.Sequential([
    feature_layer,
    tf.keras.layers.Dense(units=512, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(units=256, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(units=128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.05)),
    tf.keras.layers.Dense(units=64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dense(units=4, activation='softmax')  # 4 class ('price_range')
])

# Compile Model
model_nn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train model
history = model_nn.fit(train_ds, validation_data=test_ds, epochs=20)

"""## **XGBoost**

This code snippet builds, tunes, and trains an XGBoost classifier for a multi-class classification task. The steps include:

- **Building the model**: Creating an XGBoost classifier with specific settings.
- **Hyperparameter Tuning**: Specifying the hyperparameters and their ranges for grid search.
- **Grid Search CV**: Configuring the grid search with cross-validation to find the best hyperparameters.
- **Model Training**: Training the model using grid search to find the optimal hyperparameters.
- **Best Parameter**: Extracting and printing the best model and its parameters after the grid search.
"""

# Build Model XGBoost
model_xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')

# Parameter grid
param_grid = {
    'learning_rate': [0.1, 0.01, 0.001],
    'max_depth': [3, 5, 7],
    'n_estimators': [100, 200, 300],
}

# Grid search CV hyperparameter tuning
grid_search = GridSearchCV(model_xgb, param_grid, cv=5, n_jobs=-1, verbose=2)

# Train model
grid_search.fit(X_train, y_train)

# Best parameter
best_xgb = grid_search.best_estimator_
print('Best combination parameter:', grid_search.best_params_)

"""### **Features Importance**

Extract and display feature importance from the trained XGBoost model
"""

# Feature importance XGBoost
feature_importance = best_xgb.feature_importances_
feature_importance = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)
feature_importance = feature_importance.set_index('Feature')
feature_importance

"""### **Plot**"""

plt.figure(figsize=(12, 8))
sns.barplot(x=feature_importance.index, y=feature_importance['Importance'], palette="viridis")
plt.title('Feature Importances in XGBoost Model')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.xticks(rotation=45)
plt.show()

top_features = feature_importance.head(5)
print("Top 5 Important Features:")
print(top_features)

"""# **MODEL EVALUATION**

## **NEURAL NETWORK**

### **Accuracy**
"""

accuracy = model_nn.evaluate(test_ds)
print(f'Accuracy: {accuracy}')

"""### **Predict**

The steps in making predictions are as follows:
- Input data is passed into the network to be processed step by step.
- Forward propagation, where the input data will be processed in a forward direction through each network to produce an output.
- The softmax activation function is applied to the output of the final dense layer to get probabilities for each class.
- The predicted class is the one with the highest probability.
"""

# Predict
y_pred_probs_nn = model_nn.predict(test_ds)
y_pred_nn = tf.argmax(y_pred_probs_nn, axis=1)

y_pred_probs_nn

y_pred_nn

"""### **Evaluation Metric**"""

# Evaluation Metric
print(f'Accuracy: {accuracy_score(y_test, y_pred_nn)}')
print(f'Precision: {precision_score(y_test, y_pred_nn, average="weighted")}')
print(f'Recall: {recall_score(y_test, y_pred_nn, average="weighted")}')
print(f'F1-Score: {f1_score(y_test, y_pred_nn, average="weighted")}')

"""### **Confussion Matrix**"""

# Confusion Matrix
cm_nn = confusion_matrix(y_test, y_pred_nn)
print("Confusion Matrix NN:")
print(cm_nn)

"""### **Classification Report**"""

# Classification Report
cr_nn = classification_report(y_test, y_pred_nn)
print("\nClassification Report Neural Network:")
print(cr_nn)

"""### **ROC Score**"""

# ROC Score
roc_auc_scores = roc_auc_score(y_test, y_pred_probs_nn, multi_class='ovo')
print("ROC AUC Score:", roc_auc_scores)

"""### **Plot ROC**"""

# Plot ROC
plt.figure()
colors = ['blue', 'red', 'green', 'orange']
for i in range(4):
    fpr, tpr, _ = roc_curve(y_test, y_pred_probs_nn[:, i], pos_label=i)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, color=colors[i], lw=2, label=f'Class {i} (area = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Neural Network')
plt.legend(loc='lower right')
plt.show()

"""## **XGBOOST**

### **Predict**

The process involves making predictions using each tree in the ensemble and then aggregating these predictions. The steps in making predictions are as follows:
- For each sample in X_test, each tree makes a prediction.
- XGBoost uses boosting, which means each tree's prediction is added to the previous trees' predictions.
- After all tree predictions are added, the final prediction is obtained. If the model is for classification, this final value is used to determine the class label (softmax function).
"""

# Predict
y_pred_xgb = best_xgb.predict(X_test)
y_pred_xgb

"""### **Probability**"""

# Probability
y_pred_probs_xgb = best_xgb.predict_proba(X_test)
print(y_pred_probs_xgb)

"""### **Evaluation Metric**"""

# Evaluation Metric
print(f'Accuracy: {accuracy_score(y_test, y_pred_xgb)}')
print(f'Precision: {precision_score(y_test, y_pred_xgb, average="weighted")}')
print(f'Recall: {recall_score(y_test, y_pred_xgb, average="weighted")}')
print(f'F1-Score: {f1_score(y_test, y_pred_xgb, average="weighted")}')

"""### **Confussion Matrix**"""

# Confusion Matrix
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
print("Confusion Matrix XGBOOST:")
print(cm_xgb)

"""### **Classification Report**"""

# Classification Report
cr_xgb = classification_report(y_test, y_pred_xgb)
print("\nClassification Report XGBOOST:")
print(cr_xgb)

"""### **ROC Score**"""

# ROC Score
xgb_roc_auc = roc_auc_score(y_test, y_pred_probs_xgb, multi_class='ovr')
print(f'ROC AUC Score: {xgb_roc_auc}')

"""### **Plot ROC**"""

fpr = {}
tpr = {}
roc_auc = {}

for i in range(4):  # 4 CLASS
    fpr[i], tpr[i], _ = roc_curve(y_test, y_pred_probs_xgb[:, i], pos_label=i)
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC
plt.figure()
colors = ['blue', 'red', 'green', 'orange']
for i in range(4):
    plt.plot(fpr[i], tpr[i], color=colors[i], lw=2, label=f'Class {i} (area = {roc_auc[i]:0.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC XGBOOST')
plt.legend(loc="lower right")
plt.show()

"""# **PREDICTION**

## **Read Data**
"""

data_testing = pd.read_csv("test.csv")

"""## **Data Preprocessing**

### **Standardize**
"""

def scale_data(data_testing):
    scaled_data_testing = data_testing.copy()
    num_fit = ['battery_power', 'clock_speed', 'fc', 'int_memory', 'm_dep', 'mobile_wt',
               'n_cores', 'pc', 'px_height', 'px_width', 'ram', 'sc_h', 'talk_time']
    scaler = StandardScaler()
    scaled_data_testing[num_fit] = scaler.fit_transform(scaled_data_testing[num_fit])
    return scaled_data_testing

scaled_data_testing = scale_data(data_testing)
print(scaled_data_testing)

"""## **Neural Network**

### **Data Convertion**
"""

def data_to_dataset(dataframe, batch_size=32):
    dataframe = dataframe.copy()
    ds = tf.data.Dataset.from_tensor_slices(dict(dataframe))
    ds = ds.batch(batch_size)
    return ds

test_ds = data_to_dataset(scaled_data_testing)

"""### **Predict**"""

y_pred_probs_nn = model_nn.predict(test_ds)
y_pred_nn = tf.argmax(y_pred_probs_nn, axis=1).numpy()

print("Neural Network Predict:")
print(y_pred_nn)

"""## **XGBOOST**

### **Preparing Data**
"""

data_predict = scaled_data_testing[['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc',
                                    'four_g', 'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height', 'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g', 'touch_screen', 'wifi']]

data_predict.head()

"""### **Predict**"""

y_pred_xgb = best_xgb.predict(data_predict)

print("XGBoost Predict:")
print(y_pred_xgb)