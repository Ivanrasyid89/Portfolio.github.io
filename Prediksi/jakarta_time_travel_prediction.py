# -*- coding: utf-8 -*-
"""Jakarta Time Travel Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l3tz9Gxv_UpqjipLeVdvD9szBqbf6FFx

# **IMPORT LIBRARY**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import ttest_ind
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

"""# **DATA**

## **Read Data**
"""

# Train data
train = pd.read_csv('train_sample.csv', sep=',')
train

test = pd.read_csv('test_sample.csv', sep=',')
test

"""## **Data Information**"""

train.info()

"""Berdasarkan output, terdapat 4 kolom dengan data yang hilang.

- traffic_condition (25599/40000)
- vehicle_density (25622/40000)
- population_density (25552/40000)
- weather (25571/40000)
"""

test.info()

"""Berdasarkan output, terdapat 4 kolom dengan data yang hilang.

- traffic_condition (2400/3000)
- vehicle_density  (2400/3000)
- population_density  (2400/3000)
- weather  (2400/3000)

# **DATA PREPARATION**

## **Missing Values**

Melakukan penanganan data yang hilang pada kolom "traffic_condition" menggunakan teknik imputasi median.
"""

# Imputation "trafic_condition" for train and test
median_tc = train['traffic_condition'].median()

train['traffic_condition'].fillna(median_tc, inplace=True)
test['traffic_condition'].fillna(median_tc, inplace=True)

"""Melakukan penanganan data yang hilang pada kolom-kolom kategorik menggunakan teknik imputasi modus."""

# Imputation "vehicle_density", "population_density", "weather" for train and test
col = ['vehicle_density', 'population_density', 'weather']

for i in col:
  mode = train[i].mode()[0]
  train[i].fillna(mode, inplace=True)
  test[i].fillna(mode, inplace=True)

train.info()

test.info()

"""## **Cleaning Data**"""

import re

"""Melakukan pembersihan data untuk mengekstrak isi dalam tanda kurung () pada kolom "start_point" dan "end_point"."""

# Def function for extraction
def extract_inside_parentheses(text):
    match = re.search(r'\((.*?)\)', str(text))
    return match.group(1) if match else text  # fallback to the real text

# Apply function
train['start_point'] = train['start_point'].apply(extract_inside_parentheses)
train['end_point'] = train['end_point'].apply(extract_inside_parentheses)

# Apply function
test['start_point'] = test['start_point'].apply(extract_inside_parentheses)
test['end_point'] = test['end_point'].apply(extract_inside_parentheses)

"""# **EXPLORATORY DATA ANALYSIS**

## **Descriptive Statistics**

Menampilkan statistik 5 serangkai untuk melihat gambaran awal data yang bertipe numerik.
"""

train.describe()

"""## **Correlation**

Menampilkan plot korelasi pearson pada variabel prediktor yang bertipe numerik terhadap variabel respon untuk melihat hubungan antar variabel.
"""

# Num cols
num_cols = train.select_dtypes(include=['int64', 'float64']).columns

# Corr plot
plt.figure(figsize=(12, 8))
sns.heatmap(train[num_cols].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

"""Melakukan uji signifikansi menggunakan statistik uji T untuk mengetahui signifikansi hubungan antara variabel prediktor dan respon. Tujuannya untuk membantu menentukan variabel mana yang penting untuk prediksi."""

num_cols = train.select_dtypes(include=['int64', 'float64']).columns

# T test
for i in num_cols:
    t_stat, p_val = ttest_ind(train[i], train['travel_time'], equal_var=False)
    print(f"{i}: t-statistic = {t_stat:.5f}, p-value = {p_val:.5f}")

"""Berdasarkan output di atas, keseluruhan variabel prediktor berpengaruh signifikan terhadap respon. Artinya, kenaikkan/penurunan suatu variabel prediktor akan menaikkan/menurunkan variabel respon."""

from scipy.stats import chi2_contingency

"""Mengevaluasi hubungan antara variabel-variabel kategorikal terhadap variabel respon menggunakan statistik uji Chi-Square. Tujuannya untuk membantu menentukan variabel mana yang penting untuk prediksi."""

# Copy data train
train_copy = train.copy()

# Discretization target
train_copy['travel_time'] = pd.qcut(train['travel_time'], q=3, labels=['low', 'medium', 'high'])

# Categorical col
cat_cols = train.select_dtypes(include=['object']).columns

# P-value
for i in cat_cols:
    contingency_table = pd.crosstab(train[i], train['travel_time'])
    _, p, _, _ = chi2_contingency(contingency_table)
    print(f"{i}: p-value = {p:.5f}")

"""Berdasarkan output di atas, nilai-p keseluruhan variabel-variabel kategorik lebih kecil daripada batas signifikansi (5%). Artinya keseluruhan variabel prediktor berpengaruh signifikan terhadap respon.

## **Distributions**

Melihat distribusi data secara grafis/ilustratif menggunakan box-plot untuk mengetahui perbedaan rata-rata tiap "time_travel" di setiap kategori.
"""

cat_cols = [
    'start_point',
    'end_point',
    'time_of_day',
    'day_of_week',
    'weather',
    'vehicle_density',
    'population_density'
]

# Boxplot for each cat cols vs travel time
plt.figure(figsize=(40, 40))
for i, col in enumerate(cat_cols):
    plt.subplot(4, 3, i+1)
    sns.boxplot(x=col, y='travel_time', data=train)
    plt.xlabel(col)
    plt.ylabel('travel_time')
    plt.xticks(rotation=45)
    plt.xticks(rotation=45, fontsize=12)
    plt.yticks(fontsize=12)
plt.tight_layout()
plt.show()

"""## **Outlier Detection**

Pemeriksaan outlier pada variabel respon menggunakan box-plot.
"""

sns.boxplot(x=train['travel_time'])
plt.title("Outlier check on travel_time")
plt.show()

# Calculate Q1, 13
Q1 = train['travel_time'].quantile(0.25)
Q3 = train['travel_time'].quantile(0.75)

# Calculate IQR
IQR = Q3 - Q1

# LB and UB
LB = Q1 - 1.5 * IQR
UB = Q3 + 1.5 * IQR
print(f"Lower Bound: {LB}")
print(f"Upper Bound: {UB}")

# Detect sum and percentage outlier
outlier_sum = train[(train['travel_time'] < LB) | (train['travel_time'] > UB)].shape[0]
outlier_percentage = outlier_sum / train.shape[0] * 100
print(f"Number of outlier: {outlier_sum}")
print(f"Percentage of outlier: {outlier_percentage:.2f}%")

# copy data
train_copy = train.copy()

train_copy['travel_time'] = np.log1p(train_copy['travel_time'])

sns.boxplot(x=train_copy['travel_time'])
plt.title("Distribution of Log Travel Time")
plt.show()

"""## **Hypothesis Testing**"""

import scipy.stats as stats

cat_cols = train.select_dtypes(include=['object']).columns

# ONE WAY ANOVA
for i in cat_cols:
    group_means = train.groupby(i)['travel_time'].mean()
    f_statistic, p_value = stats.f_oneway(*[group['travel_time'] for _, group in train.groupby(i)])
    print(f"{i}: f-statistic = {f_statistic:.5f}, p-value = {p_value:.5f}")

"""## **Post-Hoc Test**"""

from statsmodels.stats.multicomp import pairwise_tukeyhsd

cat_sig_cols = [
    'start_point',
    'end_point',
    'time_of_day',
    'day_of_week',
    'vehicle_density',
    'population_density'
]

# Tukey test
for i in cat_sig_cols:
  print(f"Results for {i}:")
  tukey_result = pairwise_tukeyhsd(endog=train['travel_time'], groups=train[i], alpha=0.05)
  print(tukey_result.summary())

"""# **DATA PREPROCESSING**

## **Split Data**
"""

X = train.drop(['travel_time'], axis=1)
y = np.log1p(train['travel_time'])

# Train test split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_val: {X_val.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_val: {y_val.shape}")

"""## **Label Encoder and Normalization**"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

num_cols = [
    'traffic_condition',
    'event_count',
    'historical_delay_factor'
]

cat_cols = [
    'start_point',
    'end_point',
    'time_of_day',
    'day_of_week',
    'vehicle_density',
    'population_density',
    'weather'
]

preprocessor = ColumnTransformer([
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
])

"""# **MODELLING**"""

from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

"""## **Linear Regression**"""

# Define pipeline
lr_pipeline = Pipeline([
    ('preprocessing', preprocessor),
    ('model', LinearRegression())
])

# Define params
lr_params = {
    'model__fit_intercept': [True, False]
}

# Grid search
lr_grid = GridSearchCV(lr_pipeline, lr_params, cv=5, scoring='r2')
lr_grid.fit(X_train, y_train)
print("Best Parameters: ", lr_grid.best_params_)
print("Best Score: ", lr_grid.best_score_)

# Best model
lr_best_model = lr_grid.best_estimator_

# R2
y_pred_lr_log = lr_best_model.predict(X_val)
y_pred_lr = np.expm1(y_pred_lr_log)
print("R2 Score: ", r2_score(np.expm1(y_val), y_pred_lr))

"""## **Ridge Regression**"""

from sklearn.linear_model import Ridge

# Define pipeline
ridge_pipeline = Pipeline([
    ('preprocessing', preprocessor),
    ('model', Ridge()),
])

# Define params
ridge_params = {
    'model__alpha': [0.01, 0.1, 1, 10, 100],
    'model__fit_intercept': [True, False],
    'model__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],
    'model__max_iter': [100, 200],
    'model__tol': [1e-4]
}

# Grid search
ridge_grid = GridSearchCV(ridge_pipeline, ridge_params, cv=5, scoring='r2', n_jobs=-1)
ridge_grid.fit(X_train, y_train)
print("Best Parameters: ", ridge_grid.best_params_)
print("Best Score: ", ridge_grid.best_score_)

# Best model
ridge_best_model = ridge_grid.best_estimator_

# R2
y_pred_rr_log = ridge_best_model.predict(X_val)
y_pred_rr = np.expm1(y_pred_rr_log)
print("R2 Score: ", r2_score(np.expm1(y_val), y_pred_rr))

"""## **Lasso Regression**"""

from sklearn.linear_model import Lasso

# Define pipeline
lasso_pipeline = Pipeline([
    ('preprocessing', preprocessor),
    ('model', Lasso())
])

# Define params
lasso_params = {
    'model__alpha': [0.01, 0.1, 1, 10, 100],
    'model__fit_intercept': [True, False],
    'model__max_iter': [1000, 2000],
    'model__tol': [1e-4, 1e-5, 1e-6]
}

# Grid search
lasso_grid = GridSearchCV(lasso_pipeline, lasso_params, cv=5, scoring='r2', n_jobs=-1)
lasso_grid.fit(X_train, y_train)
print("Best Parameters: ", lasso_grid.best_params_)
print("Best Score: ", lasso_grid.best_score_)

# Best model
lasso_best_model = lasso_grid.best_estimator_

# R2
y_pred_ls_log = lasso_best_model.predict(X_val)
y_pred_ls = np.expm1(y_pred_ls_log)
print("R2 Score: ", r2_score(np.expm1(y_val), y_pred_ls))

"""## **ElasticNet**"""

from sklearn.linear_model import ElasticNet

# Define pipeline
elasticnet_pipeline = Pipeline([
    ('preprocessing', preprocessor),
    ('model', ElasticNet())
])

# Define params
elasticnet_params = {
    'model__alpha': [0.01, 0.1, 1, 10, 100],
    'model__l1_ratio': [0.1, 0.5, 0.9],
    'model__fit_intercept': [True, False],
    'model__max_iter': [1000, 2000, 3000, 5000],
    'model__tol': [1e-4, 1e-5, 1e-6]
}

# Grid search
elasticnet_grid = GridSearchCV(elasticnet_pipeline, elasticnet_params, cv=5, scoring='r2', n_jobs=-1)
elasticnet_grid.fit(X_train, y_train)
print("Best Parameters: ", elasticnet_grid.best_params_)
print("Best Score: ", elasticnet_grid.best_score_)

# Best model
elasticnet_best_model = elasticnet_grid.best_estimator_

# R2
y_pred_en_log = elasticnet_best_model.predict(X_val)
y_pred_nn = np.expm1(y_pred_en_log)
print("R2 Score: ", r2_score(np.expm1(y_val), y_pred_nn))

"""## **SGD Regression**"""

from sklearn.linear_model import SGDRegressor

# Define pipeline
sgd_pipeline = Pipeline([
    ('preprocessing', preprocessor),
    ('model', SGDRegressor(random_state=42))
])

# Define params
sgd_params = {
    'model__loss': ['huber'],
    'model__penalty': ['l2', 'l1', 'elasticnet'],
    'model__alpha': [0.01, 0.1, 1, 10, 100],
    'model__fit_intercept': [True, False],
    'model__max_iter': [1000, 2000],
    'model__tol': [1e-4, 1e-5, 1e-6],
    'model__learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive']
}

# Grid search
sgd_grid = GridSearchCV(sgd_pipeline, sgd_params, cv=5, scoring='r2', n_jobs=-1)
sgd_grid.fit(X_train, y_train)
print("Best Parameters: ", sgd_grid.best_params_)
print("Best Score: ", sgd_grid.best_score_)

# Best model
sgd_best_model = sgd_grid.best_estimator_

# R2
y_pred_sgd_log = sgd_best_model.predict(X_val)
y_pred_sgd = np.expm1(y_pred_sgd_log)
print("R2 Score: ", r2_score(np.expm1(y_val), y_pred_sgd))

"""## **Table Comparison**"""

model = ['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'ElasticNet', 'SGD Regression']

best_score = [lr_grid.best_score_*100, ridge_grid.best_score_*100, lasso_grid.best_score_*100, elasticnet_grid.best_score_*100, sgd_grid.best_score_*100]

best_r2 = [r2_score(np.expm1(y_val), y_pred_lr)*100,
           r2_score(np.expm1(y_val), y_pred_rr)*100,
           r2_score(np.expm1(y_val), y_pred_ls)*100,
           r2_score(np.expm1(y_val), y_pred_nn)*100,
           r2_score(np.expm1(y_val), y_pred_sgd)*100]

data = {
    'Model': model,
    'Best Score': best_score,
    'Best R2': best_r2
}

df = pd.DataFrame(data)
df

"""# **PREDICTION**"""

test

y_test_pred_lr_log = lr_best_model.predict(test)
y_test_pred_lr = np.expm1(y_test_pred_lr_log)
y_test_pred_lr

# save to csv
pd.DataFrame({
    'id': test.index,
    'travel_time': y_test_pred_lr
}).to_csv('submission.csv', index=False)